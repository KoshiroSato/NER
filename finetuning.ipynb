{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3065,"status":"ok","timestamp":1642928077278,"user":{"displayName":"佐藤幸志朗","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgoMU83ZAZJgQEozdHGPLTn6SnabsUzHb-WQBIBpg=s64","userId":"03692871971235645955"},"user_tz":-540},"id":"LyTjMQIdJiUJ"},"outputs":[],"source":["!pip install -q -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"mymoMwWuipoy"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3514,"status":"ok","timestamp":1642928080789,"user":{"displayName":"佐藤幸志朗","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgoMU83ZAZJgQEozdHGPLTn6SnabsUzHb-WQBIBpg=s64","userId":"03692871971235645955"},"user_tz":-540},"id":"is2bnd6-JkIZ"},"outputs":[],"source":["import gc\n","import logging\n","import warnings\n","import itertools\n","import multiprocessing\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import GroupKFold\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import pytorch_lightning as pl\n","from pytorch_lightning.utilities.seed import seed_everything\n","from transformers import (\n","    BertForTokenClassification, \n","    BertJapaneseTokenizer, \n","    get_linear_schedule_with_warmup\n","    )\n","\n","warnings.filterwarnings('ignore')\n","logging.getLogger('pytorch_lightning').setLevel(logging.ERROR)\n","logging.getLogger('transformers').setLevel(logging.ERROR)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1642928080790,"user":{"displayName":"佐藤幸志朗","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgoMU83ZAZJgQEozdHGPLTn6SnabsUzHb-WQBIBpg=s64","userId":"03692871971235645955"},"user_tz":-540},"id":"KUD9LR-eJsFE"},"outputs":[],"source":["class Cfg:\n","    debug = False\n","    seed = 42\n","    epochs = 5\n","    lr = 1e-5\n","    weight_decay = 1e-2\n","    max_len = 193\n","    n_folds = 5\n","    num_entities = 8\n","    train_batch_size = 32\n","    val_batch_size = 256\n","    group_col = 'curid'\n","    label_col = 'label'\n","    model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking' \n","    n_gpus = torch.cuda.device_count()\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1642928080790,"user":{"displayName":"佐藤幸志朗","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgoMU83ZAZJgQEozdHGPLTn6SnabsUzHb-WQBIBpg=s64","userId":"03692871971235645955"},"user_tz":-540},"id":"OszU7NraNiHV","outputId":"8be16bf4-4b0a-469d-a45f-025836c21a04"},"outputs":[{"data":{"text/plain":["42"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["seed_everything(Cfg.seed, workers=True)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1642928080790,"user":{"displayName":"佐藤幸志朗","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgoMU83ZAZJgQEozdHGPLTn6SnabsUzHb-WQBIBpg=s64","userId":"03692871971235645955"},"user_tz":-540},"id":"NAT6E8mdJy7R"},"outputs":[],"source":["data = pd.read_csv(\n","    'data/preprocessed_data.csv',\n","    dtype={\n","        'curid': object,\n","        'text_body': object,\n","        'text': object,\n","        'label': np.int32\n","    })"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1642928080790,"user":{"displayName":"佐藤幸志朗","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgoMU83ZAZJgQEozdHGPLTn6SnabsUzHb-WQBIBpg=s64","userId":"03692871971235645955"},"user_tz":-540},"id":"14VX3MPHORHV"},"outputs":[],"source":["if Cfg.debug:\n","    data = data.sample(1000, random_state=Cfg.seed)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1642928080791,"user":{"displayName":"佐藤幸志朗","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgoMU83ZAZJgQEozdHGPLTn6SnabsUzHb-WQBIBpg=s64","userId":"03692871971235645955"},"user_tz":-540},"id":"OCMr_3ZRFGnC"},"outputs":[],"source":["def init_logger(file_path):\n","    logger = logging.getLogger(__name__)\n","    logger.setLevel(logging.INFO)\n","    stream_handler = logging.StreamHandler()\n","    stream_handler.setFormatter(logging.Formatter('%(message)s'))\n","    file_handler = logging.FileHandler(filename=file_path)\n","    file_handler.setFormatter(logging.Formatter('%(message)s'))\n","    logger.addHandler(stream_handler)\n","    logger.addHandler(file_handler)\n","    return logger"]},{"cell_type":"markdown","metadata":{"id":"w53EOiJFis4Z"},"source":["## Create Dataset"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1642928080791,"user":{"displayName":"佐藤幸志朗","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgoMU83ZAZJgQEozdHGPLTn6SnabsUzHb-WQBIBpg=s64","userId":"03692871971235645955"},"user_tz":-540},"id":"3lfwMoHZJ8iR"},"outputs":[],"source":["class NERTokenizer(BertJapaneseTokenizer):\n","    def bio_tagger(self, text, label, num_entities):\n","        '''\n","        IO法でラベリングされているテキストに、\n","        トークナイズと合わせて、BIO法を適用する\n","        '''\n","        tokens = self.tokenize(text)\n","        if label > 0:\n","            labels = [label + num_entities] * len(tokens)\n","            labels[0] = label\n","        else:\n","            labels = [0] * len(tokens)\n","        return tokens, labels\n","\n","    def encode_plus_tagged(self, text, label, max_length, num_entities):\n","        '''\n","        トークナイズ結果に合わせてラベル付けをし、\n","        エンコーディング\n","        '''\n","        token_arr, label_arr = [], []\n","        tokens, labels = self.bio_tagger(text, label, num_entities)\n","        token_arr.extend(tokens)\n","        label_arr.extend(labels)\n","\n","        input_ids = self.convert_tokens_to_ids(token_arr)\n","        encoded = self.prepare_for_model(input_ids,\n","                                         max_length=max_length,\n","                                         padding='max_length',\n","                                         truncation=True)\n","        # [CLS], [SEP], [PAD]のラベルを0として追加\n","        label_arr = [0] + label_arr[:max_length-2] + [0]\n","        encoded['labels'] = label_arr + [0] * (max_length - len(label_arr))\n","        return encoded\n","\n","    def encode_plus_untagged(self, text_body, text, max_length):\n","        '''\n","        トークナイズとスパン取得を行い、\n","        エンコーディング\n","        '''\n","        tokens, tokens_for_spans = [], []\n","        words = self.word_tokenizer.tokenize(text)\n","        for word in words:\n","            subwords = self.subword_tokenizer.tokenize(word)\n","            tokens.extend(subwords)\n","            if subwords[0] == '[UNK]':\n","                tokens_for_spans.append(word)\n","            else:\n","                tokens_for_spans.extend([subword.replace('##','') for subword in subwords])\n","\n","        pos = 0\n","        spans = []\n","        for token in tokens_for_spans:\n","            token_len = len(token)\n","            while True:\n","                if token != text_body[pos:pos+token_len]:\n","                    pos += 1\n","                else:\n","                    spans.append([pos, pos+token_len])\n","                    pos += token_len\n","                    break\n","\n","        input_ids = self.convert_tokens_to_ids(tokens)\n","        encoded = self.prepare_for_model(input_ids,\n","                                         max_length=max_length,\n","                                         padding='max_length',\n","                                         truncation=True)\n","        # [CLS], [SEP], [PAD]に対応するスパン追加\n","        n_seq = len(encoded['input_ids'])\n","        spans = [[-1, -1]] + spans[:n_seq-2]\n","        spans = spans + [[-1, -1]] * (n_seq - len(spans))\n","        \n","        encoded = {k: torch.tensor([v]) for k, v in encoded.items()}\n","        return encoded, spans\n","\n","    @staticmethod\n","    def viterbi_optimizer(preds, num_entities, penalty=10000):\n","        '''\n","        BIO法のルールに従わない予測ラベル列に、\n","        ペナルティを与えて、予測値を最適化する\n","        '''\n","        m = 2 * num_entities + 1\n","        penalty_matrix = np.zeros([m,m])\n","        for i in range(m):\n","            for j in range(num_entities+1, m):\n","                if not ((i == j) or (num_entities+i == j)):\n","                    penalty_matrix[i,j] = penalty\n","        \n","        path = [[i] for i in range(m)]\n","        preds_path = preds[0] - penalty_matrix[0,:]\n","        preds = preds[1:]\n","\n","        for pred in preds:\n","            assert len(pred) == 2 * num_entities + 1\n","            pred_matrix = np.array(preds_path).reshape(-1,1) + np.array(pred).reshape(1,-1)\n","            pred_matrix -= penalty_matrix\n","            preds_path = pred_matrix.max(axis=0)\n","            pred_argmax = pred_matrix.argmax(axis=0)\n","            path = [path[idx]+[i] for i, idx in enumerate(pred_argmax)]\n","\n","        optimized_preds = path[np.argmax(preds_path)]\n","        return optimized_preds\n","\n","    def convert_bert_output_to_entities(self, text_body, preds, spans, num_entities):\n","        '''\n","        同じラベルが連続するトークンをまとめて、\n","        固有表現として抽出する\n","        '''\n","        assert len(spans) == len(preds)\n","        # [CLS], [SEP], [PAD]に対応する箇所を削除\n","        preds = [pred for pred, span in zip(preds, spans) if span[0] != -1]\n","        spans = [span for span in spans if span[0] != -1]\n","        preds = self.viterbi_optimizer(preds, num_entities)\n","        \n","        entities = []\n","        for pred, group in itertools.groupby(enumerate(preds), key=lambda x: x[1]):\n","            group = list(group)\n","            start = spans[group[0][0]][0]\n","            end = spans[group[-1][0]][1]\n","\n","            if pred != 0:\n","                # Bならば\n","                if 1 <= pred <= num_entities:\n","                    entity = {\n","                        'name': text_body[start:end],\n","                        'span': [start, end],\n","                        'type_id': pred\n","                    }\n","                    entities.append(entity)\n","                # Iならば\n","                else:\n","                    entity['span'][1] = end\n","                    entity['name'] = text_body[entity['span'][0]:entity['span'][1]]\n","        return entities"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":615,"status":"ok","timestamp":1642928081402,"user":{"displayName":"佐藤幸志朗","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgoMU83ZAZJgQEozdHGPLTn6SnabsUzHb-WQBIBpg=s64","userId":"03692871971235645955"},"user_tz":-540},"id":"C4-MBcwfKE19"},"outputs":[],"source":["class NERDataset(Dataset):\n","    def __init__(self, data, tokenizer, config):\n","        super().__init__()\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.config = config\n","\n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, index):\n","        data_row = self.data.iloc[index]\n","        text = data_row['text']\n","        label = data_row['label']\n","        \n","        encoded = self.tokenizer.encode_plus_tagged(\n","            text, label, self.config.max_len, self.config.num_entities\n","            )\n","        encoded = {k: torch.tensor(v) for k, v in encoded.items()}\n","        return {\n","            'input_ids': encoded['input_ids'].flatten(),\n","            'token_type_ids': encoded['token_type_ids'].flatten(),\n","            'attention_mask': encoded['attention_mask'].flatten(),\n","            'labels': encoded['labels'].flatten()\n","        }\n","\n","\n","class NERDataModule(pl.LightningDataModule):\n","    def __init__(self, train_data, val_data, tokenizer, config):\n","        super().__init__()\n","        self.train_data = train_data\n","        self.val_data = val_data\n","        self.tokenizer = tokenizer\n","        self.config = config\n","\n","    def create_dataset(self, mode):\n","        return (\n","            NERDataset(self.train_data, self.tokenizer, self.config)\n","            if mode == 'train'\n","            else NERDataset(self.val_data, self.tokenizer, self.config)\n","        )\n","\n","    def train_dataloader(self):\n","        train_ds = self.create_dataset(mode='train')\n","        train_loader = DataLoader(train_ds,\n","                                  batch_size=self.config.train_batch_size,\n","                                  num_workers=multiprocessing.cpu_count(),\n","                                  pin_memory=True,\n","                                  drop_last=True,\n","                                  shuffle=True)\n","        return train_loader\n","\n","    def val_dataloader(self):\n","        val_ds = self.create_dataset(mode='val')\n","        val_loader = DataLoader(val_ds,\n","                                batch_size=self.config.val_batch_size,\n","                                num_workers=multiprocessing.cpu_count(),\n","                                pin_memory=True,\n","                                drop_last=False,\n","                                shuffle=False)\n","        return val_loader"]},{"cell_type":"markdown","metadata":{"id":"fzEZ-0Gxiwsy"},"source":["## Create Model\n","\n","#### BERT Tagger"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1642928081402,"user":{"displayName":"佐藤幸志朗","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgoMU83ZAZJgQEozdHGPLTn6SnabsUzHb-WQBIBpg=s64","userId":"03692871971235645955"},"user_tz":-540},"id":"aY9KIB5aOYW3"},"outputs":[],"source":["class NERModel(pl.LightningModule):\n","    def __init__(self, config, num_training_steps):\n","        super().__init__()\n","        self.config = config\n","        self.num_training_steps = num_training_steps\n","        self.bert = BertForTokenClassification.from_pretrained(\n","            self.config.model_name,\n","            num_labels=2 * self.config.num_entities + 1\n","            )\n","        self.criterion = nn.CrossEntropyLoss()\n","        \n","    def forward(self, input_ids, token_type_ids, attention_mask, labels=None):\n","        preds = self.bert(\n","            input_ids, \n","            token_type_ids=token_type_ids, \n","            attention_mask=attention_mask\n","            )\n","        if labels is not None:\n","            loss = self.criterion(preds, labels)\n","            return loss\n","        return  preds\n","    \n","    def training_step(self, batch, batch_idx):\n","        output = self.bert(**batch)\n","        loss = output.loss\n","        self.log('train_loss', loss)\n","        return loss\n","    \n","    def validation_step(self, batch, batch_idx):\n","        output = self.bert(**batch)\n","        val_loss = output.loss\n","        self.log('val_loss', val_loss)\n","        \n","    def configure_optimizers(self):\n","        no_decay = ['bias', 'LayerNorm.weight']\n","        optimizer_grouped_parameters = [\n","            {\n","                'params': [p for n, p in self.bert.named_parameters()\n","                            if not any(nd in n for nd in no_decay)],\n","                'weight_decay': self.config.weight_decay\n","            },\n","            {\n","                'params': [p for n, p in self.bert.named_parameters()\n","                            if any(nd in n for nd in no_decay)],\n","                'weight_decay': 0.0\n","            }\n","        ]\n","        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=self.config.lr)\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=0,\n","            num_training_steps=self.num_training_steps\n","        )\n","        return [optimizer], [scheduler]"]},{"cell_type":"markdown","metadata":{"id":"2uFBD9sii0XB"},"source":["## FineTuning -> Inference -> Evaluation"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1642928081403,"user":{"displayName":"佐藤幸志朗","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgoMU83ZAZJgQEozdHGPLTn6SnabsUzHb-WQBIBpg=s64","userId":"03692871971235645955"},"user_tz":-540},"id":"afZrz6Iea-GY"},"outputs":[],"source":["def ner_inference(model, val_data, tokenizer, config):\n","    all_entities, all_targets = [], []\n","    n_val = len(val_data)\n","    for i in range(n_val):\n","        encoded, spans = tokenizer.encode_plus_untagged(\n","            val_data.iloc[i]['text_body'],\n","            val_data.iloc[i]['text'], \n","            config.max_len\n","            )\n","        encoded = {k: v.to(config.device) for k, v in encoded.items()}\n","        model.to(config.device)\n","        model.eval()\n","        with torch.no_grad():\n","            output = model(**encoded)\n","        preds = output.logits[0].cpu().detach().numpy().tolist()\n","        entities = tokenizer.convert_bert_output_to_entities(\n","            val_data.iloc[i]['text_body'], \n","            preds, \n","            spans,\n","            config.num_entities\n","            )\n","        all_entities.append(entities)\n","        # モデル評価の為に、ターゲットのエンティティを作成\n","        target_name = val_data.iloc[i]['text']\n","        target_span = [val_data.iloc[i]['start'], val_data.iloc[i]['end']]\n","        target_typeId = val_data.iloc[i]['label']\n","        if target_typeId == 0:\n","            targets = []\n","        else:\n","            targets = [{'name': target_name, 'span': target_span, 'type_id': target_typeId}]\n","        all_targets.append(targets)\n","\n","    return all_entities, all_targets"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1642928081403,"user":{"displayName":"佐藤幸志朗","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgoMU83ZAZJgQEozdHGPLTn6SnabsUzHb-WQBIBpg=s64","userId":"03692871971235645955"},"user_tz":-540},"id":"0oMlSPyQmPJd"},"outputs":[],"source":["def ner_evaluation(entities_arr, targets_arr):\n","    n_entities, n_targets, n_correct = 0, 0, 0\n","    for entities, targets in zip(entities_arr, targets_arr):\n","        get_span_type = lambda x: (x['span'][0], x['span'][1], x['type_id'])\n","        set_entities = set(get_span_type(entity) for entity in entities)\n","        set_targets = set(get_span_type(target) for target in targets)\n","        n_targets += len(targets)\n","        n_entities += len(entities)\n","        n_correct += len(set_entities & set_targets)\n","    precision = n_correct / n_entities\n","    recall = n_correct / n_targets\n","    f1 = 2 * precision * recall / (precision + recall)\n","    return {'precision': precision, 'recall': recall, 'f1': f1}"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1642928081403,"user":{"displayName":"佐藤幸志朗","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgoMU83ZAZJgQEozdHGPLTn6SnabsUzHb-WQBIBpg=s64","userId":"03692871971235645955"},"user_tz":-540},"id":"3PJjvBIwOYRW"},"outputs":[],"source":["def run_train(fold, tokenizer, data, tr_idx, val_idx, logger, config):\n","    logger.info(f'\\t----- Fold: {fold} -----')\n","    train, val = data.iloc[tr_idx], data.iloc[val_idx]\n","\n","    checkpoint = pl.callbacks.ModelCheckpoint(monitor='val_loss', \n","                                              mode='min', \n","                                              save_top_k=1,  \n","                                              save_weights_only=True, \n","                                              dirpath=f'model_folds_seed_{config.seed}/model_fold{fold}/')\n","    \n","    es_callback = pl.callbacks.EarlyStopping(monitor='val_loss', \n","                                             patience=3)\n","    \n","    tb_logger = pl.loggers.TensorBoardLogger(f'model_folds_seed_{config.seed}/model_fold{fold}_logs/')\n","\n","    trainer = pl.Trainer(max_epochs=config.epochs,\n","                         gpus=config.n_gpus,\n","                         logger=tb_logger,\n","                         callbacks=[checkpoint,es_callback],\n","                         progress_bar_refresh_rate=0)\n","    \n","    num_training_steps = ((len(train)) // (config.train_batch_size)) * float(config.epochs)\n","    model = NERModel(config, num_training_steps)\n","    datamodule = NERDataModule(train, val, tokenizer, config)\n","    trainer.fit(model, datamodule=datamodule)\n","\n","    model.load_state_dict(torch.load(checkpoint.best_model_path)['state_dict'])\n","    entities_arr, targets_arr = ner_inference(model, val, tokenizer, config)\n","\n","    del datamodule, model\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    return entities_arr, targets_arr"]},{"cell_type":"markdown","metadata":{"id":"2Vx4k_SDi31H"},"source":["## Metrics\n","#### Precision, Recall, F1\n","## CV\n","#### GroupKFold\n","`curid`をGroup IDとして、 バリデーション分割をする"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1642928081403,"user":{"displayName":"佐藤幸志朗","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgoMU83ZAZJgQEozdHGPLTn6SnabsUzHb-WQBIBpg=s64","userId":"03692871971235645955"},"user_tz":-540},"id":"gDUkcl3gPL3f"},"outputs":[],"source":["def run_all(config):\n","    gkf = GroupKFold(n_splits=config.n_folds)\n","    tokenizer = NERTokenizer.from_pretrained(config.model_name)\n","    logger = init_logger('cv_results/cv.log')\n","    precision_score = 0.0\n","    recall_score = 0.0\n","    f1_score = 0.0\n","\n","    for i, (tr_idx, val_idx) in enumerate(gkf.split(data, data[config.label_col], data[config.group_col])):\n","        entities_arr, targets_arr = run_train(i, tokenizer, data, tr_idx, val_idx, logger, config)\n","        eval_result = ner_evaluation(entities_arr, targets_arr)\n","        precision_score += eval_result['precision']\n","        recall_score += eval_result['recall']\n","        f1_score += eval_result['f1']\n","        \n","        logger.info(f'FOLD{i} PRECISION SCORE: {eval_result[\"precision\"]:.5f}')\n","        logger.info(f'FOLD{i} RECALL SCORE: {eval_result[\"recall\"]:.5f}')\n","        logger.info(f'FOLD{i} F1 SCORE: {eval_result[\"f1\"]:.5f}')\n","    logger.info(f'{config.n_folds}FOLDS PRECISION CV SCORE: {precision_score/config.n_folds:.5f}')\n","    logger.info(f'{config.n_folds}FOLDS RECALL CV SCORE: {recall_score/config.n_folds:.5f}')\n","    logger.info(f'{config.n_folds}FOLDS F1 CV SCORE: {f1_score/config.n_folds:.5f}')"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12570832,"status":"ok","timestamp":1642940652231,"user":{"displayName":"佐藤幸志朗","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgoMU83ZAZJgQEozdHGPLTn6SnabsUzHb-WQBIBpg=s64","userId":"03692871971235645955"},"user_tz":-540},"id":"Pqg2A0uodThd","outputId":"34989f62-131b-4450-829f-2315b4f69de2"},"outputs":[{"name":"stderr","output_type":"stream","text":["\t----- Fold: 0 -----\n","FOLD0 PRECISION SCORE: 0.76272\n","FOLD0 RECALL SCORE: 0.76243\n","FOLD0 F1 SCORE: 0.76257\n","\t----- Fold: 1 -----\n","FOLD1 PRECISION SCORE: 0.75396\n","FOLD1 RECALL SCORE: 0.75681\n","FOLD1 F1 SCORE: 0.75538\n","\t----- Fold: 2 -----\n","FOLD2 PRECISION SCORE: 0.75427\n","FOLD2 RECALL SCORE: 0.75656\n","FOLD2 F1 SCORE: 0.75541\n","\t----- Fold: 3 -----\n","FOLD3 PRECISION SCORE: 0.77144\n","FOLD3 RECALL SCORE: 0.77407\n","FOLD3 F1 SCORE: 0.77275\n","\t----- Fold: 4 -----\n","FOLD4 PRECISION SCORE: 0.77386\n","FOLD4 RECALL SCORE: 0.77416\n","FOLD4 F1 SCORE: 0.77401\n","5FOLDS PRECISION CV SCORE: 0.76325\n","5FOLDS RECALL CV SCORE: 0.76481\n","5FOLDS F1 CV SCORE: 0.76403\n"]}],"source":["run_all(Cfg)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"finetuning.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
